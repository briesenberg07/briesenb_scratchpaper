{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# from RDF dump, get ONS IRIs which don't exist in ONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## do this for just one collection at a time\n",
    "*this is just a little hack script*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rdflib\n",
    "\n",
    "# to-dos\n",
    "    # !! speed this up !!\n",
    "        # sort by vocab and only parse each vocab once for that group???\n",
    "    # report on time taken?\n",
    "    # add coll name (from dump file) to report top line and txt filename\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2720 distinct ONS values found\n"
     ]
    }
   ],
   "source": [
    "# get all ONS.org values\n",
    "filepath = input(\"Enter RDF dump filename with extension (must be in data_scratch/colls/)\\n>>> \")\n",
    "\n",
    "all_distinct_ons = []\n",
    "g = rdflib.Graph().parse(f\"../../data_scratch/colls/{filepath}\")\n",
    "result = g.query(\n",
    "    \"\"\"\n",
    "    SELECT DISTINCT ?ons_resource\n",
    "    WHERE {\n",
    "        ?s ?p ?ons_resource .\n",
    "        FILTER CONTAINS (str(?ons_resource), \"http://opaquenamespace.org/ns/\")\n",
    "        }\n",
    "    \"\"\")\n",
    "counter = 0\n",
    "for row in result:\n",
    "    counter += 1\n",
    "    # print(row.ons_resource)\n",
    "    all_distinct_ons.append(str(row.ons_resource))\n",
    "print(f\"{counter} distinct ONS values found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot retrieve a label for 67 ONS resources\n"
     ]
    }
   ],
   "source": [
    "# iterate through these to see which fail when attempting to retrieve label\n",
    "    # this is the part that I'd think needs to be sped up\n",
    "    # by for example, parsing each ONS vocab only once for terms from that vocab\n",
    "failures = []\n",
    "for iri in all_distinct_ons:\n",
    "    try:\n",
    "        g = rdflib.Graph().parse(f\"{iri}.nt\")\n",
    "    except:\n",
    "        failures.append(iri)\n",
    "print(f\"Cannot retrieve a label for {len(failures)} ONS resources\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "coll_alias = filepath.split('/')[-1]\n",
    "coll_alias = coll_alias.split('.')[0]\n",
    "\n",
    "with open(f\"ref_results/{coll_alias}_failures.json\", 'w+') as jsonfile:\n",
    "    json.dump({f\"{coll_alias}_failures\": failures}, jsonfile)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## put together a combined list from all collection results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "<class 'list'>\n",
      "1 failed ONS IRIs in collection angelus\n",
      "2\n",
      "<class 'list'>\n",
      "187 failed ONS IRIs in collection building-or\n",
      "3\n",
      "<class 'list'>\n",
      "9 failed ONS IRIs in collection chinavine\n",
      "4\n",
      "<class 'list'>\n",
      "0 failed ONS IRIs in collection fealy\n",
      "5\n",
      "<class 'list'>\n",
      "22 failed ONS IRIs in collection lowenstam\n",
      "6\n",
      "<class 'list'>\n",
      "2 failed ONS IRIs in collection marketing-photos\n",
      "7\n",
      "<class 'list'>\n",
      "1 failed ONS IRIs in collection nosatsu\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "colls = ['angelus', 'building-or', 'chinavine', 'fealy', 'lowenstam', \n",
    "         'marketing-photos', 'nosatsu', 'uo-arch-photos']\n",
    "counter = 0\n",
    "combined_failures = []\n",
    "for coll in colls:\n",
    "    counter += 1\n",
    "    with open(f\"ref_results/{coll}_failures.json\", \"r\") as jsonfile:\n",
    "        faildict = json.load(jsonfile)\n",
    "        faillist = faildict[f\"{coll}_failures\"]\n",
    "        print(counter)\n",
    "        # print(type(faillist))\n",
    "        print(f\"{len(faillist)} failed ONS IRIs in collection {coll}\")\n",
    "        if counter <= 1:\n",
    "            for fail in faillist:\n",
    "                combined_failures.append(fail)\n",
    "        else:\n",
    "            for fail in faillist:\n",
    "                if fail not in combined_failures:\n",
    "                    combined_failures.append(fail)\n",
    "                else:\n",
    "                    pass\n",
    "with open(\"ref_results/combined_faillist.json\", \"w+\") as jsonfile:\n",
    "    json.dump({\"combined_faillist\": faillist}, jsonfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
